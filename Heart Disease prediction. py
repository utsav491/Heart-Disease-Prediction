import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import rcParams

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier

df =pd.read_csv('C:\\Users\\utsav\\Desktop\\Heart-Disease-Prediction\\dataset.csv')
df.head()

df.shape
rcParams['figure.figsize'] = 20, 14

df.info()

df.describe()

fig, ax = plt.subplots(figsize = (10,10))

sns.heatmap(df.corr() , annot = True , ax = ax , cmap = "YlGnBu" )


df.hist()

rcParams['figure.figsize'] = 6,4
sns.countplot(x = df["target"] )

rcParams['figure.figsize'] = 6,4
sns.countplot(x = df["target"]  , hue =df["sex"], palette="bwr")

from sklearn.preprocessing import StandardScaler
standardscaler = StandardScaler()
columns_which_are_to_be_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
df[columns_to_scale] = standardScaler.fit_transform(df[columns_which_are_to_be_scale])
print(df.head())
df.shape


X = df.iloc[:, 0:13]
y = df.iloc[:, 13]

X = df.iloc[:,0:13].values
y = df.iloc[:,13].values

X_train , X_test , y_train , y_test = train_test_split(X, y , test_size = 0.33 , random_state  = 114)

accuracies_of_all_models = {}


""" Classification Through K Nearest Neighbors"""
import matplotlib.pyplot as plt
score = []
for i in range(1,20):
    knn = KNeighborsClassifier(n_neighbors = i)
    knn.fit(X_train, y_train)
    y_pred_knn = knn.predict(X_test)  
    score.append(accuracy_score(y_test ,y_pred_knn))
up  = [c for c in score]
score_knn = max(up)
print(score_knn)
accuracies_of_all_models["KNN"] = score_knn
plt.plot([k for k in range(1, 20)], up, color = 'green')
#plt.plot(x = [w   for w  in range(1,21)] , y = up)
plt.xticks(np.arange(1,20,1))
plt.xlabel("K value")
plt.ylabel("Score")
plt.show()


""" Confusion Matrix  and Classification Report of Decision Tree"""
import seaborn as sns
from sklearn.metrics import confusion_matrix , classification_report
sns.heatmap(cm , cmap = 'Blues', annot = True , linewidth  =0.5, xticklabels = ["Has Disease", "No Disease"] , yticklabels = ["Has Disease", "No Disease"],fmt="d",cbar=False)
cm = confusion_matrix(y_test, y_pred_knn)
clf_report = classification_report(y_test, y_pred_knn)
print(clf_report)



""" Classification through Logistic Regression"""
logreg = LogisticRegression()
logreg.fit(X_train ,y_train)
y_pred_logreg = logreg.predict(X_test)
# Compute predicted probabilities: y_pred_prob
y_pred_prob = logreg.predict_proba(X_test)[:,1]
#print(y_pred_prob)
logreg_score = accuracy_score(y_pred,y_test)
print(logreg_score)
accuracies_of_all_models["Logistic Regression"] = logreg_score

""" Confusion Matrix  and Classification Report of Logistic Regression"""
import seaborn as sns
from sklearn.metrics import confusion_matrix , classification_report
sns.heatmap(cm , cmap = 'Blues', annot = True , linewidth  =0.5, xticklabels = ["Has Disease", "No Disease"] , yticklabels = ["Has Disease", "No Disease"],fmt="d",cbar=False)
cm = confusion_matrix(y_test, y_pred_logreg)
clf_report = classification_report(y_test, y_pred_logreg)
print(clf_report)


""" Classification through Decision Tree"""
dt = DecisionTreeClassifier()
dt.fit(X_train ,y_train)
prediction_dt = dt.predict(X_test)
decision_tree_score  = accuracy_score(prediction_dt, y_test)
print(decision_tree_score)
accuracies_of_all_models["Decision Tree"] = decision_tree_score

""" Confusion Matrix  and Classification Report of Decision Tree"""
import seaborn as sns
from sklearn.metrics import confusion_matrix , classification_report
sns.heatmap(cm , cmap = 'Blues', annot = True , linewidth  =0.5, xticklabels = ["Has Disease", "No Disease"] , yticklabels = ["Has Disease", "No Disease"],fmt="d",cbar=False)
cm = confusion_matrix(y_test, prediction_dt)
clf_report = classification_report(y_test,prediction_dt)
print(clf_report)



#Ensemble Learning
knn = KNeighborsClassifier(n_neighbors=3)

dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state= 42)

lr = LogisticRegression(random_state = 42)
classifiers = [('Logistic Regression' , lr) ,('K Nearest Neighbors' , knn) , ('Decision Tree' , dt)]

for clf_name , clf in classifiers:
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        test_score = accuracy_score(y_pred , y_test)
        print('{:s} : {:.3f}'.format(clf_name, test_score))
        
 from sklearn.ensemble import VotingClassifier

dt = DecisionTreeClassifier(min_samples_leaf=0.13)
lr = LogisticRegression()
knn = KNeighborsClassifier(n_neighbors = 5)
classifier = [('logistic regression', lr), ('K nearest neighbors' , knn), ('Decision Tree' ,dt)]

for clf_name, clf in classifier:
    clf.fit(X_train , y_train)
    y_pred = clf.predict(X_test)
    score = accuracy_score(y_test ,y_pred)
    print("The {}:{}".format(clf_name, score))

vc = VotingClassifier(estimators = classifier)
vc.fit(X_train , y_train)
y_pred = vc.predict(X_test)
acc  = accuracy_score(y_test , y_pred)
print("Voting Classifier Accuracy is {}".format(acc))



colors = [ "green","blue","orange"]

sns.set_style("whitegrid")
plt.figure(figsize=(16,5))
plt.yticks(np.arange(0,100,10))
plt.ylabel("Accuracy %")
plt.xlabel("Algorithms")
sns.barplot(x=list(accuracies_of_all_models.keys()), y=list(accuracies_of_all_models.values()), palette=colors)
plt.show()
